{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes taken from:\n",
    "    \n",
    "Learning Path: Deep Dive into Python Machine Learning, presented by Eder Santana, Chapter 4\n",
    "    \n",
    "by Ankita Thakur - Curator\n",
    "\n",
    "Published by Packt Publishing, 2016\n",
    "\n",
    "Learn what is Deep Learning, Recurrent Neural Networks --Training a Sentiment Analysis Model for Text, using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.datasets import imdb  # import dataset\n",
    "\n",
    "from theano import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis code was borrowed and modified from https://github.com/fchollet\\n\\nTrain a LSTM on the IMDB sentiment classificaiton task\\nThe dataset is acually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg\\nNotes:\\n    -RNNs are tricky. Choice of batch size is important,\\n    choice of loss and optimzer is critical, etc.\\n    Some configurations won't converge.\\n    -LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc.\\n    GPU command:\\n        THEANO_FLAGS=mode-FAST_RUN,device=gpu,floatX=float32 python imdb\\n        \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This code was borrowed and modified from https://github.com/fchollet\n",
    "\n",
    "Train a LSTM on the IMDB sentiment classification task\n",
    "The dataset is acually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg\n",
    "Notes:\n",
    "    -RNNs are tricky. Choice of batch size is important,\n",
    "    choice of loss and optimzer is critical, etc.\n",
    "    Some configurations won't converge.\n",
    "    -LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc.\n",
    "    GPU command:\n",
    "        THEANO_FLAGS=mode-FAST_RUN,device=gpu,floatX=float32 python imdb\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100 # cut texts after this number of words (among top max_features)\n",
    "batch_size = 32\n",
    "test_split = 0.2\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)  # test_split=0.2 did not work with version; test_split left out is set to 50/50\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (Samples x time)\n",
      "X_train shape: (25000, 100)\n",
      "X_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad sequences (Samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSample reviews from the full IMDb movie reviews dataset.\\n\\nNegative review examples:\\n* Unfortunately it stays absurd the WHOLE time with no general narrative\\n* Even those from the era should be turned off.\\n* The cryptic dialogue would make Shakespeaere seem easy to a third grader\\n\\nPositive review examples:\\n* I didn't know this came from Canada, but it is very good. Very good!\\n* I liked this movie a lot.  It really intrigued me how Deanna and Alicia\\n* When I saw the elaborate DVD box for this and the dreadful Red Queen f\\n  I felt certain I was in for a big disappointment, but surprise, surprise\\n  \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sample reviews from the full IMDb movie reviews dataset.\n",
    "\n",
    "Negative review examples:\n",
    "* Unfortunately it stays absurd the WHOLE time with no general narrative\n",
    "* Even those from the era should be turned off.\n",
    "* The cryptic dialogue would make Shakespeaere seem easy to a third grader\n",
    "\n",
    "Positive review examples:\n",
    "* I didn't know this came from Canada, but it is very good. Very good!\n",
    "* I liked this movie a lot.  It really intrigued me how Deanna and Alicia\n",
    "* When I saw the elaborate DVD box for this and the dreadful Red Queen f\n",
    "  I felt certain I was in for a big disappointment, but surprise, surprise\n",
    "  \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1415,    33,     6,    22,    12,   215,    28,    77,    52,\n",
       "           5,    14,   407,    16,    82, 10311,     8,     4,   107,\n",
       "         117,  5952,    15,   256,     4,     2,     7,  3766,     5,\n",
       "         723,    36,    71,    43,   530,   476,    26,   400,   317,\n",
       "          46,     7,     4, 12118,  1029,    13,   104,    88,     4,\n",
       "         381,    15,   297,    98,    32,  2071,    56,    26,   141,\n",
       "           6,   194,  7486,    18,     4,   226,    22,    21,   134,\n",
       "         476,    26,   480,     5,   144,    30,  5535,    18,    51,\n",
       "          36,    28,   224,    92,    25,   104,     4,   226,    65,\n",
       "          16,    38,  1334,    88,    12,    16,   283,     5,    16,\n",
       "        4472,   113,   103,    32,    15,    16,  5345,    19,   178,    32], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]  # ID of each word, encode each word as a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# Need word to vector representation\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# other options\n",
    "#See on github piskvorky/gensim  python API for word to vector\n",
    "# See on github stanfordnlp/GloVe  for word to vector\n",
    "\n",
    "model.add(LSTM(128)) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.5/site-packages/keras/models.py:517: UserWarning: \"class_mode\" argument is deprecated, please remove it.\n",
      "  warnings.warn('\"class_mode\" argument is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                class_mode=\"binary\",\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = model.input\n",
    "embedding = model.layers[0].output\n",
    "F = function([inp], embedding, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Input:\n",
      "[[ 1415    33     6    22    12   215    28    77    52     5    14   407\n",
      "     16    82 10311     8     4   107   117  5952    15   256     4     2\n",
      "      7  3766     5   723    36    71    43   530   476    26   400   317\n",
      "     46     7     4 12118  1029    13   104    88     4   381    15   297\n",
      "     98    32  2071    56    26   141     6   194  7486    18     4   226\n",
      "     22    21   134   476    26   480     5   144    30  5535    18    51\n",
      "     36    28   224    92    25   104     4   226    65    16    38  1334\n",
      "     88    12    16   283     5    16  4472   113   103    32    15    16\n",
      "   5345    19   178    32]]\n",
      ">> Input shape:\n",
      "(1, 100)\n",
      ">> Embedding:\n",
      "[[[ 0.00810685  0.09325636 -0.07744114 ...,  0.07493792  0.09282391\n",
      "   -0.02799342]\n",
      "  [-0.02806837  0.00239264 -0.02356007 ...,  0.03474515  0.01536963\n",
      "   -0.07854667]\n",
      "  [-0.05315669 -0.03417091 -0.02350992 ..., -0.02584297 -0.03199266\n",
      "   -0.0728862 ]\n",
      "  ..., \n",
      "  [-0.0020438  -0.02712085 -0.03039971 ..., -0.0190105   0.04001851\n",
      "    0.00341527]\n",
      "  [-0.0174851   0.05927418 -0.08593485 ...,  0.00854491  0.02717509\n",
      "   -0.11574145]\n",
      "  [-0.03809026  0.05402245 -0.02157982 ...,  0.03867733  0.01176692\n",
      "    0.00020078]]]\n",
      ">> Embedding shape:\n",
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\">> Input:\")\n",
    "print(X_train[:1])\n",
    "print(\">> Input shape:\")\n",
    "print(X_train[:1].shape)\n",
    "print(\">> Embedding:\")\n",
    "print(F(X_train[:1]))\n",
    "print(\">> Embedding shape:\")\n",
    "print(F(X_train[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 273s - loss: 0.0700 - acc: 0.9769 - val_loss: 0.6092 - val_acc: 0.8269\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 261s - loss: 0.0351 - acc: 0.9892 - val_loss: 0.7231 - val_acc: 0.8292\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 256s - loss: 0.0250 - acc: 0.9928 - val_loss: 0.7151 - val_acc: 0.8235\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 246s - loss: 0.0175 - acc: 0.9948 - val_loss: 0.8622 - val_acc: 0.8284\n",
      "25000/25000 [==============================] - 49s    \n",
      "Test score: 0.862169817462\n",
      "Test accuracy: 0.82844\n"
     ]
    }
   ],
   "source": [
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size,\n",
    "         nb_epoch=4, validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Final Test results is for the held-out data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go to the Keras examples folder to see IMDb for more examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
