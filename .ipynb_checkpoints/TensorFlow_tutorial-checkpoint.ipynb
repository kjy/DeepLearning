{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hello, TensorFlow! \n",
    " Building and training your first TensorFlow graph from the ground up\n",
    "\n",
    "Oriole Online Tutorial by Aaron Schumacher, September 8, 2016\n",
    "\n",
    "https://www.safaribooksonline.com/oriole/\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google open-sourced TensorFlow package in November 2015.\n",
    "\n",
    "Tensors: matrix, multi-dimensional approach to numerical computation, focus is on weights rather than neurons\n",
    "\n",
    "Flows:  manages computations, graph with nodes and edges where data moves through the graph and computation happens at the nodes\n",
    "    \n",
    "Easy to put computation on to high performance GPUs (parallel processing, distributed computation)\n",
    "\n",
    "Tensor boards:  system for logging and visualizing what you are doing; built-in easy to get statistics (less manual like in sci-kit learn)\n",
    "    \n",
    "High performance serving infrastructure to serve TensorFlow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`foo == bar` evaluates to: True\n",
      "`foo is bar` evaluates to: True\n"
     ]
    }
   ],
   "source": [
    "# Name managed by tensorflow system\n",
    "foo = []\n",
    "bar = foo\n",
    "#  these are not 2 different lists; 2 names point to the same list.\n",
    "#  an object in python has no name\n",
    "# The variable names in Python code aren't what they represent; \n",
    "# they're just pointing at the same object.\n",
    "# name is separate from the object itself. Name points to object.\n",
    "\n",
    "print('`foo == bar` evaluates to: {}'.format(foo == bar))\n",
    "print('`foo is bar` evaluates to: {}'.format(foo is bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`id(foo)` evaluates to: 4367939528\n",
      "`id(bar)` evaluates to: 4367939528\n"
     ]
    }
   ],
   "source": [
    "# You can also see that id(foo) and id(bar) are the same.\n",
    "print('`id(foo)` evaluates to: {}'.format(id(foo)))\n",
    "print('`id(bar)` evaluates to: {}'.format(id(bar)))\n",
    "\n",
    "# This identity, especially with mutable data structures like lists, \n",
    "# can lead to surprising bugs when it's misunderstood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Python displays `foo` as: [[...]]\n"
     ]
    }
   ],
   "source": [
    "# Take a list and append it inside of itself\n",
    "# How should python interpret this? Infinite recurrent data structure.\n",
    "# Continues to point to itself. Python gives up.\n",
    "# Imagine this as a graph. \n",
    "\n",
    "foo.append(bar)\n",
    "print('Now Python displays `foo` as: {}'.format(foo))\n",
    "\n",
    "# It keeps repeating and can't show you the whole thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow separates the definition of computations from their execution \n",
    "even further by having them happen in separate places: a graph defines the\n",
    "operations, but the operations only happen within a session. Graphs and \n",
    "sessions are created independently. A graph is like a blueprint and a \n",
    "session is like a construction site.\n",
    "\n",
    "Back to our plain Python example, recall that foo and bar refer to the same list. By appending bar into foo, we've put a list inside itself. You could think of this structure as a graph with one node, pointing to itself. Nesting lists is one way to represent a graph structure like a TensorFlow computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point TensorFlow has already started managing a lot of state for \n",
    "us. There's already an implicit default graph, for example. Internally, \n",
    "the default graph lives in the _default_graph_stack, but we don't have \n",
    "access to that directly. We use tf.get_default_graph()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign name so that we can refer to it.\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the TensorFlow graph are called “operations” or “ops”. We can \n",
    "see what operations are in the graph with graph.get_operations()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operations()\n",
    "# Nothing has happened because we did not put anything into the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there isn't anything in the graph. We’ll need to put everything \n",
    "we want TensorFlow to compute into that graph. Let's start with a simple \n",
    "constant input value of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign name so that we can refer to it.\n",
    "input_value = tf.constant(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`operations` is now: [<tensorflow.python.framework.ops.Operation object at 0x10ffdae80>]\n"
     ]
    }
   ],
   "source": [
    "operations = graph.get_operations()\n",
    "print('`operations` is now: {}'.format(operations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow uses protocol buffers internally. (Protocol buffers are sort of \n",
    "like a Google-strength JSON.) Printing the node_def for the constant operation above shows what's in TensorFlow's protocol buffer representation for the number one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"Const\"\n",
       "op: \"Const\"\n",
       "attr {\n",
       "  key: \"dtype\"\n",
       "  value {\n",
       "    type: DT_FLOAT\n",
       "  }\n",
       "}\n",
       "attr {\n",
       "  key: \"value\"\n",
       "  value {\n",
       "    tensor {\n",
       "      dtype: DT_FLOAT\n",
       "      tensor_shape {\n",
       "      }\n",
       "      float_val: 1.0\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations[0].node_def\n",
    "# double float data type; json format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People new to TensorFlow sometimes wonder why there's all this fuss about \n",
    "making “TensorFlow versions” of things. Why can't we just use a normal \n",
    "Python variable without also defining a TensorFlow object? One of the \n",
    "TensorFlow tutorials has an explanation:\n",
    "\n",
    "To do efficient numerical computing in Python, we typically use libraries \n",
    "like NumPy that do expensive operations such as matrix multiplication \n",
    "outside Python, using highly efficient code implemented in another \n",
    "language. Unfortunately, there can still be a lot of overhead from \n",
    "switching back to Python every operation. This overhead is especially bad \n",
    "if you want to run computations on GPUs or in a distributed manner, where \n",
    "there can be a high cost to transferring data.\n",
    "\n",
    "TensorFlow also does its heavy lifting outside Python, but it takes things \n",
    "a step further to avoid this overhead. Instead of running a single expensive operation independently from Python, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. This approach is similar to that used in Theano or Torch.\n",
    "\n",
    "TensorFlow can do a lot of great things, but it can only work with what's \n",
    "been explicitly given to it. This is true even for a single constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we inspect our input_value, we see it is a constant 32-bit float tensor \n",
    "# of no dimension: just one number.\n",
    "\n",
    "input_value\n",
    "\n",
    "# It doesn't tell you the value because you did not ask it to evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this doesn't tell us what that number is. To evaluate input_value and get a numerical value out, we need to create a “session” where graph operations can be evaluated and then explicitly ask to evaluate or “run” input_value. (The session picks up the default graph by default.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`input_value` evaluates to: 1.0\n"
     ]
    }
   ],
   "source": [
    "# A session is a place for where computation can happen\n",
    "# Graph is like a blueprint\n",
    "sess = tf.Session()  # session is like the construction site where you put things together\n",
    "print('`input_value` evaluates to: {}'.format(sess.run(input_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is managing its own space of things—the computational graph—\n",
    "and it has its own method of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a session with a simple graph, let's build a neuron with \n",
    "just one parameter, or weight. Often, even simple neurons also have a bias \n",
    "term and a non-identity activation function, but we'll leave these out.\n",
    "\n",
    "The neuron's weight isn't going to be constant; we expect it to change in \n",
    "order to learn, based on the “true” input and output we use for training. \n",
    "The weight will be a TensorFlow variable. We'll give that variable a \n",
    "starting value of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Simplest TensorFlow Neuron\n",
    "weight = tf.Variable(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might expect that adding a variable would add one operation to the \n",
    "graph, but in fact that one line adds four operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const\n",
      "Variable/initial_value\n",
      "Variable\n",
      "Variable/Assign\n",
      "Variable/read\n"
     ]
    }
   ],
   "source": [
    "for op in graph.get_operations(): \n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_value = weight * input_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are six operations in the graph, and the last one is that multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `op.name` is: mul\n"
     ]
    }
   ],
   "source": [
    "op = graph.get_operations()[-1]\n",
    "print('The `op.name` is: {}'.format(op.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable/read:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for op_input in op.inputs: \n",
    "    print(op_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how the multiplication operation tracks where its inputs come \n",
    "from: they come from other operations in the graph. To understand a whole \n",
    "graph, following references this way quickly becomes tedious for humans. \n",
    "TensorBoard graph visualization is designed to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find out what the product is? We have to “run” the output_value \n",
    "operation. But that operation depends on a variable: weight. We told TensorFlow that the initial value of weight should be 0.8, but the value hasn't yet been set in the current session. The tf.initialize_all_variables() function generates an operation which will initialize all our variables (in this case just one) and then we can run that operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of tf.initialize_all_variables() will include initializers for \n",
    "all the variables currently in the graph, so if you add more variables \n",
    "you'll want to use tf.initialize_all_variables() again; a stale init \n",
    "wouldn't include the new variables. Calling tf.initialize_all_variables()\n",
    "many times won't hurt anything in the examples here, but it does add to \n",
    "the graph each time it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we're ready to run the output_value operation.\n",
    "sess.run(output_value)\n",
    "\n",
    "# Why isn't it 0.8 exactly? Why the extras?\n",
    "# TensorFlow cares about types. \n",
    "# Recall that's 0.8 * 1.0 with 32-bit floats, and 32-bit floats have a hard time with 0.8; \n",
    "# 0.80000001 is as close as they can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See your graph in Tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the default graph, you may want to clear it out from time to \n",
    "time rather than just keep adding more and more things to it. Here we \n",
    "reset the graph and start a new session using that new empty graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, the graph has been simple, but it would already be nice \n",
    "to see it represented in a diagram. We'll use TensorBoard to generate that \n",
    "diagram. TensorBoard reads the name field that is stored inside each \n",
    "operation (quite distinct from Python variable names). We can use these \n",
    "TensorFlow names and switch to more conventional Python variable names. \n",
    "Using tf.mul here is equivalent to our earlier use of just * for \n",
    "multiplication, but it lets us set the name for the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1.0, name='input')\n",
    "w = tf.Variable(0.8, name='weight')\n",
    "y = tf.mul(w, x, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard works by looking at a directory of output created from TensorFlow sessions. Since TensorFlow will keep appending to an existing directory of output, which can lead to confusion in interpreting results, you'll usually want to ensure you're starting from an empty directory. If you're running locally, you may want to run rm -rf log_simple_graph at a shell prompt (not in the Python interpreter) or in a Jupyter notebook with a leading exclamation point, like !rm -rf log_simple_graph. In this Oriole, we'll just show the relevant visualizations from TensorBoard.\n",
    "\n",
    "The log output for TensorBoard can be written with a SummaryWriter, and if \n",
    "we do nothing aside from creating one with a graph, it will just write out \n",
    "that graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first argument when creating the SummaryWriter is an output directory name, \n",
    "# which will be created if it doesn't exist.\n",
    "summary_writer = tf.train.SummaryWriter('log_simple_graph', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running locally, you can start up TensorBoard at the command line by running tensorboard --logdir=log_simple_graph. This runs a local web app on port 6006. (“6006” is “goog” upside-down.) After startup, go in a web browser to http://localhost:6006/#graphs to see the interface. You'll see a diagram of the graph you created in TensorFlow, as in Figure 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve built our neuron, how does it learn? We set up an input value of 1.0. Let's say the correct output value is zero. That is, we have a very simple “training set” of just one example with one feature, which has the value one, and one label, which is zero. We want the neuron to learn the function taking one to zero.\n",
    "\n",
    "Currently, the system takes the input one and returns 0.8, which is not \n",
    "correct. We need a way to measure how wrong the system is. We'll call \n",
    "that measure of wrongness the “loss” and give our system the goal of \n",
    "minimizing the loss. If the loss can be negative, then minimizing it \n",
    "could be silly, so let's make the loss the square of the difference \n",
    "between the current output and the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.constant(0.0)\n",
    "loss = (y - y_)**2  # squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, nothing in the graph does any learning. For that, we need an \n",
    "optimizer. We'll use a gradient descent optimizer so that we can update \n",
    "the weight based on the derivative of the loss. The optimizer takes a \n",
    "learning rate to moderate the size of the updates, which we'll set at \n",
    "0.025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = tf.train.GradientDescentOptimizer(learning_rate=0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is remarkably clever. It can automatically work out and apply the appropriate gradients through a whole network, carrying out the backward step for learning.\n",
    "\n",
    "Let's see what the gradient looks like for our simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is: 1.600000023841858\n"
     ]
    }
   ],
   "source": [
    "grads_and_vars = optim.compute_gradients(loss)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print('The gradient is: {}'.format(sess.run(grads_and_vars[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the value of the gradient 1.6? Our loss is error squared, and the \n",
    "derivative of that is two times the error. Currently the system says 0.8 \n",
    "instead of 0, so the error is 0.8, and two times 0.8 is 1.6. It's working!\n",
    "\n",
    "For more complex systems, it will be very nice indeed that TensorFlow \n",
    "calculates and then applies these gradients for us automatically.\n",
    "\n",
    "Let's apply the gradient, finishing the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new value of the weight is: 0.7599999904632568\n"
     ]
    }
   ],
   "source": [
    "sess.run(optim.apply_gradients(grads_and_vars))\n",
    "print('The new value of the weight is: {}'.format(sess.run(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight decreased by 0.04 because the optimizer subtracted the gradient \n",
    "times the learning rate, 1.6 * 0.025, pushing the weight in the right \n",
    "direction.\n",
    "\n",
    "Instead of hand-holding the optimizer like this, we can make one operation \n",
    "that calculates and applies the gradients: the train_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neuron now outputs: 0.004499601200222969\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n",
    "\n",
    "for i in range(100):\n",
    "    sess.run(train_step)\n",
    "\n",
    "print('The neuron now outputs: {}'.format(sess.run(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the training step many times, the weight and the output value are \n",
    "now very close to zero. The neuron has learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING DIAGNOSTICS IN TENSORBOARD\n",
    "\n",
    "We may be interested in what's happening during training. Say we want to \n",
    "follow what our system is predicting at every training step. We could \n",
    "print from inside the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before step 0, y is 0.800000011920929\n",
      "before step 1, y is 0.7599999904632568\n",
      "before step 2, y is 0.722000002861023\n",
      "before step 3, y is 0.6858999729156494\n",
      "before step 4, y is 0.651604950428009\n",
      "before step 5, y is 0.6190246939659119\n",
      "before step 6, y is 0.5880734324455261\n",
      "before step 7, y is 0.5586697459220886\n",
      "before step 8, y is 0.5307362675666809\n",
      "before step 9, y is 0.5041994452476501\n",
      "before step 10, y is 0.47898948192596436\n",
      "before step 11, y is 0.45504000782966614\n",
      "before step 12, y is 0.4322880208492279\n",
      "before step 13, y is 0.4106736183166504\n",
      "before step 14, y is 0.39013993740081787\n",
      "before step 15, y is 0.37063294649124146\n",
      "before step 16, y is 0.35210129618644714\n",
      "before step 17, y is 0.33449622988700867\n",
      "before step 18, y is 0.31777140498161316\n",
      "before step 19, y is 0.3018828332424164\n",
      "before step 20, y is 0.2867887020111084\n",
      "before step 21, y is 0.272449254989624\n",
      "before step 22, y is 0.2588267922401428\n",
      "before step 23, y is 0.2458854466676712\n",
      "before step 24, y is 0.23359116911888123\n",
      "before step 25, y is 0.22191160917282104\n",
      "before step 26, y is 0.21081602573394775\n",
      "before step 27, y is 0.2002752274274826\n",
      "before step 28, y is 0.19026146829128265\n",
      "before step 29, y is 0.18074838817119598\n",
      "before step 30, y is 0.17171096801757812\n",
      "before step 31, y is 0.1631254255771637\n",
      "before step 32, y is 0.15496915578842163\n",
      "before step 33, y is 0.1472207009792328\n",
      "before step 34, y is 0.1398596614599228\n",
      "before step 35, y is 0.13286668062210083\n",
      "before step 36, y is 0.1262233406305313\n",
      "before step 37, y is 0.11991217732429504\n",
      "before step 38, y is 0.11391656845808029\n",
      "before step 39, y is 0.10822074115276337\n",
      "before step 40, y is 0.10280970484018326\n",
      "before step 41, y is 0.09766922146081924\n",
      "before step 42, y is 0.09278576076030731\n",
      "before step 43, y is 0.08814647048711777\n",
      "before step 44, y is 0.08373914659023285\n",
      "before step 45, y is 0.07955218851566315\n",
      "before step 46, y is 0.07557457685470581\n",
      "before step 47, y is 0.07179585099220276\n",
      "before step 48, y is 0.0682060569524765\n",
      "before step 49, y is 0.06479575484991074\n",
      "before step 50, y is 0.06155596673488617\n",
      "before step 51, y is 0.05847816914319992\n",
      "before step 52, y is 0.055554259568452835\n",
      "before step 53, y is 0.05277654528617859\n",
      "before step 54, y is 0.0501377172768116\n",
      "before step 55, y is 0.047630831599235535\n",
      "before step 56, y is 0.04524929076433182\n",
      "before step 57, y is 0.04298682510852814\n",
      "before step 58, y is 0.04083748534321785\n",
      "before step 59, y is 0.03879561275243759\n",
      "before step 60, y is 0.03685583174228668\n",
      "before step 61, y is 0.03501303866505623\n",
      "before step 62, y is 0.03326238691806793\n",
      "before step 63, y is 0.031599268317222595\n",
      "before step 64, y is 0.030019305646419525\n",
      "before step 65, y is 0.02851833961904049\n",
      "before step 66, y is 0.027092423290014267\n",
      "before step 67, y is 0.02573780156672001\n",
      "before step 68, y is 0.024450911208987236\n",
      "before step 69, y is 0.023228365927934647\n",
      "before step 70, y is 0.02206694707274437\n",
      "before step 71, y is 0.020963599905371666\n",
      "before step 72, y is 0.019915420562028885\n",
      "before step 73, y is 0.018919650465250015\n",
      "before step 74, y is 0.01797366887331009\n",
      "before step 75, y is 0.017074985429644585\n",
      "before step 76, y is 0.016221236437559128\n",
      "before step 77, y is 0.015410174615681171\n",
      "before step 78, y is 0.014639666303992271\n",
      "before step 79, y is 0.013907683081924915\n",
      "before step 80, y is 0.013212298974394798\n",
      "before step 81, y is 0.012551683932542801\n",
      "before step 82, y is 0.01192410010844469\n",
      "before step 83, y is 0.0113278953358531\n",
      "before step 84, y is 0.010761500336229801\n",
      "before step 85, y is 0.01022342499345541\n",
      "before step 86, y is 0.00971225369721651\n",
      "before step 87, y is 0.009226640686392784\n",
      "before step 88, y is 0.008765308186411858\n",
      "before step 89, y is 0.008327042683959007\n",
      "before step 90, y is 0.007910690270364285\n",
      "before step 91, y is 0.0075151557102799416\n",
      "before step 92, y is 0.007139397785067558\n",
      "before step 93, y is 0.00678242789581418\n",
      "before step 94, y is 0.006443306338042021\n",
      "before step 95, y is 0.006121140904724598\n",
      "before step 96, y is 0.005815084092319012\n",
      "before step 97, y is 0.005524329841136932\n",
      "before step 98, y is 0.005248113535344601\n",
      "before step 99, y is 0.004985707812011242\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(100):\n",
    "    print('before step {}, y is {}'.format(i, sess.run(y)))\n",
    "    sess.run(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but there are some problems. It's hard to understand a list of \n",
    "numbers. A plot would be better; and even with only one value to monitor, \n",
    "there's too much output to read. We're likely to want to monitor many \n",
    "things. It would be nice to record everything in some organized way.\n",
    "\n",
    "Luckily, the same system that we used earlier to visualize the graph also \n",
    "has just the mechanisms we need.\n",
    "\n",
    "We instrument the computation graph by adding operations that summarize \n",
    "its state. Here, we'll create an operation that reports the current value \n",
    "of y, the neuron's current output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_y = tf.scalar_summary('output', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run a summary operation, it returns a string of protocol buffer \n",
    "text that can be written to a log directory with a SummaryWriter.\n",
    "\n",
    "Again, if running locally you'll want to ensure that you start with an \n",
    "empty output directory by running rm -rf log_simple_stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.train.SummaryWriter('log_simple_stat')\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(100):\n",
    "    summary_str = sess.run(summary_y)\n",
    "    summary_writer.add_summary(summary_str, i)\n",
    "    sess.run(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now TensorBoard can visualize the logged results. If running locally, \n",
    "you would run tensorboard --logdir=log_simple_stat and then go to \n",
    "http://localhost:6006/#events to see the interface. \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLOWING ONWARD\n",
    "\n",
    "Once again to freshen things, we can get a new empty default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running locally, it would be a good time to also clear out the output \n",
    "directory: rm -rf log_simple_stats.\n",
    "\n",
    "Here's a final version of the code. It's fairly minimal, with every part \n",
    "showing useful (and understandable) TensorFlow functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(1.0, name='input')\n",
    "w = tf.Variable(0.8, name='weight')\n",
    "y = tf.mul(w, x, name='output')\n",
    "y_ = tf.constant(0.0, name='correct_value')\n",
    "loss = tf.pow(y - y_, 2, name='loss')\n",
    "train_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n",
    "\n",
    "for value in [x, w, y, y_, loss]:\n",
    "    tf.scalar_summary(value.op.name, value)\n",
    "\n",
    "summaries = tf.merge_all_summaries()\n",
    "\n",
    "sess = tf.Session()\n",
    "summary_writer = tf.train.SummaryWriter('log_simple_stats', sess.graph)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(100):\n",
    "    summary_writer.add_summary(sess.run(summaries), i)\n",
    "    sess.run(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running locally, you could start up TensorBoard with \n",
    "tensorboard --logdir=log_simple_stats and then go to \n",
    "http://localhost:6006/#events to see the interface. You would see plots \n",
    "for all five instrumented scalars.\n",
    "\n",
    "The example we just ran through is even simpler than the ones that \n",
    "inspired it in Michael Nielsen's Neural Networks and Deep Learning. \n",
    "For myself, seeing details like these helps with understanding and \n",
    "building more complex systems that use and extend from simple building \n",
    "blocks. Part of the beauty of TensorFlow is how flexibly you can build \n",
    "complex systems from simpler components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
